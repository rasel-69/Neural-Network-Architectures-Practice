{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":7045013,"sourceType":"datasetVersion","datasetId":4053823}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:56:55.319405Z","iopub.execute_input":"2026-02-03T10:56:55.319731Z","iopub.status.idle":"2026-02-03T10:57:32.615820Z","shell.execute_reply.started":"2026-02-03T10:56:55.319703Z","shell.execute_reply":"2026-02-03T10:57:32.610583Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:57:32.645714Z","iopub.execute_input":"2026-02-03T10:57:32.645885Z","iopub.status.idle":"2026-02-03T10:57:32.655486Z","shell.execute_reply.started":"2026-02-03T10:57:32.645868Z","shell.execute_reply":"2026-02-03T10:57:32.650499Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Synthetic classification data\nX = np.random.rand(2000, 20).astype(np.float32)  # 20 features\ny = np.random.randint(0, 3, size=(2000,))        # 3 classes\n\nX_train, X_test = X[:1600], X[1600:]\ny_train, y_test = y[:1600], y[1600:]\n\ntrain_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\ntest_dataset  = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:58:35.238579Z","iopub.execute_input":"2026-02-03T10:58:35.238869Z","iopub.status.idle":"2026-02-03T10:58:35.252434Z","shell.execute_reply.started":"2026-02-03T10:58:35.238849Z","shell.execute_reply":"2026-02-03T10:58:35.248570Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"**##Attention Module**","metadata":{}},{"cell_type":"code","source":"class ChannelAttention(nn.Module):\n    def __init__(self, features, reduction=16):\n        super().__init__()\n\n        self.attention = nn.Sequential(\n            nn.Linear(features, features // reduction),\n            nn.ReLU(),\n            nn.Linear(features // reduction, features),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        avg_pool = torch.mean(x, dim=0, keepdim=True)\n        attn = self.attention(avg_pool)\n        return x * attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:58:38.932404Z","iopub.execute_input":"2026-02-03T10:58:38.932695Z","iopub.status.idle":"2026-02-03T10:58:38.942728Z","shell.execute_reply.started":"2026-02-03T10:58:38.932677Z","shell.execute_reply":"2026-02-03T10:58:38.938660Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Residual Connection With Attention**","metadata":{}},{"cell_type":"code","source":"class ResidualCBAMBlock(nn.Module):\n    def __init__(self, features, dropout=0.3):\n        super().__init__()\n\n        self.bn1 = nn.BatchNorm1d(features)\n        self.fc1 = nn.Linear(features, features)\n\n        self.bn2 = nn.BatchNorm1d(features)\n        self.fc2 = nn.Linear(features, features)\n\n        self.cbam = ChannelAttention(features)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n\n        out = self.fc1(self.relu(self.bn1(x)))\n        out = self.dropout(out)\n        out = self.fc2(self.relu(self.bn2(out)))\n\n        out = self.cbam(out)       # ðŸ”¥ Attention applied here\n        out = out + identity       # Residual connection\n\n        return self.relu(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:58:43.364561Z","iopub.execute_input":"2026-02-03T10:58:43.364857Z","iopub.status.idle":"2026-02-03T10:58:43.377396Z","shell.execute_reply.started":"2026-02-03T10:58:43.364837Z","shell.execute_reply":"2026-02-03T10:58:43.373158Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Deep Neural Network with Residual Connection and CBAM attention Module**","metadata":{}},{"cell_type":"code","source":"class DeepResidualCBAMNet(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n\n        self.fc_in = nn.Linear(input_size, 128)\n\n        self.res1 = ResidualCBAMBlock(128)\n        self.res2 = ResidualCBAMBlock(128)\n        self.res3 = ResidualCBAMBlock(128)  # ðŸ”¥ extra depth\n\n        self.fc1 = nn.Linear(128, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc_out = nn.Linear(32, num_classes)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc_in(x))\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.res3(x)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        return self.fc_out(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:58:58.666686Z","iopub.execute_input":"2026-02-03T10:58:58.666934Z","iopub.status.idle":"2026-02-03T10:58:58.676452Z","shell.execute_reply.started":"2026-02-03T10:58:58.666914Z","shell.execute_reply":"2026-02-03T10:58:58.673129Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = DeepResidualCBAMNet(input_size=20, num_classes=3).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:59:06.753382Z","iopub.execute_input":"2026-02-03T10:59:06.753623Z","iopub.status.idle":"2026-02-03T10:59:14.011433Z","shell.execute_reply.started":"2026-02-03T10:59:06.753598Z","shell.execute_reply":"2026-02-03T10:59:14.005672Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"epochs = 80\ntrain_losses = []\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    train_losses.append(avg_loss)\n\n    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:59:14.062634Z","iopub.execute_input":"2026-02-03T10:59:14.062942Z","iopub.status.idle":"2026-02-03T10:59:29.529027Z","shell.execute_reply.started":"2026-02-03T10:59:14.062925Z","shell.execute_reply":"2026-02-03T10:59:29.524384Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/80] - Loss: 1.0987\nEpoch [2/80] - Loss: 1.0927\nEpoch [3/80] - Loss: 1.0811\nEpoch [4/80] - Loss: 1.0643\nEpoch [5/80] - Loss: 1.0463\nEpoch [6/80] - Loss: 1.0204\nEpoch [7/80] - Loss: 1.0021\nEpoch [8/80] - Loss: 0.9657\nEpoch [9/80] - Loss: 0.9591\nEpoch [10/80] - Loss: 0.9155\nEpoch [11/80] - Loss: 0.8992\nEpoch [12/80] - Loss: 0.8706\nEpoch [13/80] - Loss: 0.8309\nEpoch [14/80] - Loss: 0.8360\nEpoch [15/80] - Loss: 0.7935\nEpoch [16/80] - Loss: 0.7611\nEpoch [17/80] - Loss: 0.7395\nEpoch [18/80] - Loss: 0.6981\nEpoch [19/80] - Loss: 0.6826\nEpoch [20/80] - Loss: 0.6746\nEpoch [21/80] - Loss: 0.6390\nEpoch [22/80] - Loss: 0.6195\nEpoch [23/80] - Loss: 0.5620\nEpoch [24/80] - Loss: 0.6255\nEpoch [25/80] - Loss: 0.5781\nEpoch [26/80] - Loss: 0.5333\nEpoch [27/80] - Loss: 0.5091\nEpoch [28/80] - Loss: 0.5118\nEpoch [29/80] - Loss: 0.5318\nEpoch [30/80] - Loss: 0.4632\nEpoch [31/80] - Loss: 0.4620\nEpoch [32/80] - Loss: 0.4348\nEpoch [33/80] - Loss: 0.4192\nEpoch [34/80] - Loss: 0.4265\nEpoch [35/80] - Loss: 0.4058\nEpoch [36/80] - Loss: 0.4094\nEpoch [37/80] - Loss: 0.4200\nEpoch [38/80] - Loss: 0.3721\nEpoch [39/80] - Loss: 0.4085\nEpoch [40/80] - Loss: 0.3588\nEpoch [41/80] - Loss: 0.3865\nEpoch [42/80] - Loss: 0.3679\nEpoch [43/80] - Loss: 0.3356\nEpoch [44/80] - Loss: 0.3264\nEpoch [45/80] - Loss: 0.3500\nEpoch [46/80] - Loss: 0.3205\nEpoch [47/80] - Loss: 0.3264\nEpoch [48/80] - Loss: 0.2866\nEpoch [49/80] - Loss: 0.2958\nEpoch [50/80] - Loss: 0.2929\nEpoch [51/80] - Loss: 0.2785\nEpoch [52/80] - Loss: 0.2630\nEpoch [53/80] - Loss: 0.2732\nEpoch [54/80] - Loss: 0.2738\nEpoch [55/80] - Loss: 0.2456\nEpoch [56/80] - Loss: 0.2409\nEpoch [57/80] - Loss: 0.2605\nEpoch [58/80] - Loss: 0.2653\nEpoch [59/80] - Loss: 0.2300\nEpoch [60/80] - Loss: 0.2493\nEpoch [61/80] - Loss: 0.2062\nEpoch [62/80] - Loss: 0.2409\nEpoch [63/80] - Loss: 0.2071\nEpoch [64/80] - Loss: 0.2261\nEpoch [65/80] - Loss: 0.2251\nEpoch [66/80] - Loss: 0.2200\nEpoch [67/80] - Loss: 0.2264\nEpoch [68/80] - Loss: 0.2041\nEpoch [69/80] - Loss: 0.2102\nEpoch [70/80] - Loss: 0.2071\nEpoch [71/80] - Loss: 0.2030\nEpoch [72/80] - Loss: 0.2053\nEpoch [73/80] - Loss: 0.2232\nEpoch [74/80] - Loss: 0.2239\nEpoch [75/80] - Loss: 0.1997\nEpoch [76/80] - Loss: 0.1924\nEpoch [77/80] - Loss: 0.1751\nEpoch [78/80] - Loss: 0.2025\nEpoch [79/80] - Loss: 0.1886\nEpoch [80/80] - Loss: 0.1846\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        preds = torch.argmax(outputs, dim=1)\n\n        y_true.extend(labels.numpy())\n        y_pred.extend(preds.cpu().numpy())\n\nprint(classification_report(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T10:59:33.825479Z","iopub.execute_input":"2026-02-03T10:59:33.825713Z","iopub.status.idle":"2026-02-03T10:59:33.856733Z","shell.execute_reply.started":"2026-02-03T10:59:33.825688Z","shell.execute_reply":"2026-02-03T10:59:33.852011Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.28      0.27      0.27       132\n           1       0.35      0.36      0.35       135\n           2       0.38      0.38      0.38       133\n\n    accuracy                           0.34       400\n   macro avg       0.33      0.33      0.33       400\nweighted avg       0.33      0.34      0.33       400\n\n","output_type":"stream"}],"execution_count":14}]}